# -*- coding: utf-8 -*-
"""
æ‰¹æ¬¡è™•ç†ï¼ˆé€æ®µé©—è­‰ â†’ åˆä½µ â†’ ç”¢å‡ºæœ€çµ‚æª”ï¼‰
"""
import os, re, json, time, hashlib
from typing import List, Tuple, Dict, Any, Optional, Callable

# âœ… ä¿®æ­£ import ç‚º modules.xx
from modules.transcribe import transcribe
from modules.analyze import run_analysis
from modules.ai_precheck import ai_precheck  # âœ… ç¢ºä¿é€™å€‹æ¨¡çµ„æ”¾åœ¨ modules/
from docx import Document
from modules.doc_generator import generate_docx

# é è¨­è³‡æ–™å¤¾
SEG_AUDIO_DIR_DEFAULT = os.path.join("output", "segments")
SEG_TEXT_DIR          = os.path.join("output", "segments_text")
FINAL_DIR             = os.path.join("output", "final")
LOGS_DIR              = os.path.join("output", "logs")

os.makedirs(SEG_TEXT_DIR, exist_ok=True)
os.makedirs(FINAL_DIR, exist_ok=True)
os.makedirs(LOGS_DIR, exist_ok=True)

def _nkey(s: str):
    return [int(t) if t.isdigit() else t for t in re.split(r"(\d+)", s)]

def _sha256(path: str) -> str:
    h = hashlib.sha256()
    with open(path, "rb") as f:
        for chunk in iter(lambda: f.read(1024 * 1024), b""):
            h.update(chunk)
    return h.hexdigest()

def _save_text(path: str, text: str):
    with open(path, "w", encoding="utf-8") as f:
        f.write(text or "")

def _save_json(path: str, data: Dict[str, Any]):
    with open(path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

def _save_docx(path: str, summary: str, outline: List[str], todos: List[str]):
    doc = Document()
    doc.add_heading("æœƒè­°æ‘˜è¦å ±å‘Š", level=0)
    doc.add_heading("æ‘˜è¦", level=1)
    doc.add_paragraph(summary if summary else "ï¼ˆç„¡å…§å®¹ï¼‰")
    doc.add_heading("å¤§ç¶±", level=1)
    if outline:
        for item in outline:
            doc.add_paragraph(item, style="List Bullet")
    else:
        doc.add_paragraph("ï¼ˆç„¡å…§å®¹ï¼‰")
    doc.add_heading("å¾…è¾¦äº‹é …", level=1)
    if todos:
        for t in todos:
            doc.add_paragraph(t, style="List Number")
    else:
        doc.add_paragraph("ï¼ˆç„¡å…§å®¹ï¼‰")
    doc.save(path)

def _list_segments(seg_dir: str) -> List[str]:
    files = [os.path.join(seg_dir, f) for f in os.listdir(seg_dir)
             if os.path.splitext(f.lower())[1] in {".wav", ".mp3", ".m4a"}]
    files.sort(key=lambda p: _nkey(os.path.basename(p)))
    return files

def _stitch(paths: List[str], header_prefix: str) -> str:
    chunks = []
    for i, p in enumerate(paths, 1):
        name = os.path.basename(p)
        with open(p, "r", encoding="utf-8") as f:
            txt = f.read()
        chunks.append(f"=== {header_prefix} {i:02d} | {name} ===\n{txt}\n")
    return "\n".join(chunks)

def run_batch_process(
    segments_dir: str = SEG_AUDIO_DIR_DEFAULT,
    preview: bool = False,
    force: bool = False,
    return_progress: bool = False,
    *,
    start_index: int = 1,
    max_segments: Optional[int] = None,
    on_progress: Optional[Callable[[int, int, str], None]] = None,
) -> str:
    t0 = time.time()
    progress: List[str] = []

    def log(msg: str):
        s = time.strftime("%H:%M:%S") + " " + msg
        print(s)
        progress.append(s)

    all_seg_files = _list_segments(segments_dir)
    if not all_seg_files:
        raise FileNotFoundError(f"åœ¨ {segments_dir} æ²’æ‰¾åˆ°ä»»ä½•éŸ³æª”ã€‚")

    start = max(0, start_index - 1)
    end = None if not max_segments or max_segments <= 0 else start + max_segments
    seg_files = all_seg_files[start:end]
    total = len(seg_files)

    log(f"ðŸ”§ å°‡è™•ç†æ®µè½ï¼šå¾žç¬¬ {start_index} æ®µé–‹å§‹ï¼›æœ€å¤šè™•ç† {max_segments if max_segments else 'å…¨éƒ¨'} æ®µã€‚")
    log(f"ðŸ“Š å¯¦éš›æ®µæ•¸ï¼š{total}ï¼ˆå…¨éƒ¨å…±æœ‰ {len(all_seg_files)} æ®µï¼‰")

    manifest = {
        "segments_dir": segments_dir,
        "created_at": time.strftime("%Y-%m-%d %H:%M:%S"),
        "preview": preview,
        "items": [],
        "start_index": start_index,
        "count": total
    }

    if on_progress:
        try: on_progress(0, total, "é–‹å§‹è™•ç†")
        except: pass

    for idx, audio_path in enumerate(seg_files, 1):
        abs_index = start_index + idx - 1
        seg_id = f"{abs_index:02d}"
        base = f"seg_{seg_id}"
        paths = {
            "transcript": os.path.join(SEG_TEXT_DIR, f"{base}_transcript.txt"),
            "review":     os.path.join(SEG_TEXT_DIR, f"{base}_review.txt"),
            "revised":    os.path.join(SEG_TEXT_DIR, f"{base}_revised.txt"),
            "meta":       os.path.join(SEG_TEXT_DIR, f"{base}_meta.json"),
        }

        if not force and all(os.path.exists(p) for p in paths.values()):
            log(f"ðŸŸ¡ è·³éŽ {base}ï¼ˆå·²æœ‰è¼¸å‡ºï¼‰")
            with open(paths["meta"], "r", encoding="utf-8") as f:
                meta = json.load(f)
            manifest["items"].append(meta)
            if on_progress:
                try: on_progress(idx, total, f"è·³éŽç¬¬ {abs_index} æ®µ")
                except: pass
            continue

        log(f"ðŸŽ§ [{seg_id}] è™•ç†ï¼š{os.path.basename(audio_path)}")
        try:
            transcript = transcribe(audio_path)
            review_text, revised_text = ai_precheck(transcript, preview=preview)

            _save_text(paths["transcript"], transcript)
            _save_text(paths["review"], review_text)
            _save_text(paths["revised"], revised_text)

            meta = {
                "seg_id": seg_id,
                "audio": os.path.basename(audio_path),
                "audio_sha256": _sha256(audio_path),
                "paths": paths,
                "lens": {
                    "transcript": len(transcript or ""),
                    "review": len(review_text or ""),
                    "revised": len(revised_text or "")
                },
                "ok": True,
            }
            _save_json(paths["meta"], meta)
            manifest["items"].append(meta)
            log(f"âœ… [{seg_id}] å®Œæˆï¼ˆT{meta['lens']['transcript']}/R{meta['lens']['revised']}ï¼‰")
        except Exception as e:
            meta = {
                "seg_id": seg_id,
                "audio": os.path.basename(audio_path),
                "paths": paths,
                "ok": False,
                "error": repr(e)
            }
            _save_json(paths["meta"], meta)
            manifest["items"].append(meta)
            log(f"âŒ [{seg_id}] å¤±æ•—ï¼š{e!r}")

        if on_progress:
            try: on_progress(idx, total, f"å®Œæˆç¬¬ {abs_index} æ®µ")
            except: pass

    manifest_path = os.path.join(LOGS_DIR, "batch_manifest.json")
    _save_json(manifest_path, manifest)

    ok_items = [it for it in manifest["items"] if it.get("ok")]
    tr_paths = [it["paths"]["transcript"] for it in ok_items]
    rvw_paths = [it["paths"]["review"]     for it in ok_items]
    rvd_paths = [it["paths"]["revised"]    for it in ok_items]

    merged_transcript = _stitch(tr_paths, "TRANSCRIPT SEG") if tr_paths else ""
    merged_review     = _stitch(rvw_paths, "REVIEW SEG")     if rvw_paths else ""
    merged_revised    = _stitch(rvd_paths, "REVISED SEG")    if rvd_paths else ""

    final_paths = {
        "transcript_txt": os.path.join(FINAL_DIR, "transcript.txt"),
        "transcript_review_txt": os.path.join(FINAL_DIR, "transcript_review.txt"),
        "transcript_revised_txt": os.path.join(FINAL_DIR, "transcript_revised.txt"),
    }
    _save_text(final_paths["transcript_txt"], merged_transcript)
    _save_text(final_paths["transcript_review_txt"], merged_review)
    _save_text(final_paths["transcript_revised_txt"], merged_revised)

    summary, outline, todos = run_analysis(merged_revised, preview=False)

    meeting_paths = {
        "txt":  os.path.join(FINAL_DIR, "meeting_summary.txt"),
        "docx": os.path.join(FINAL_DIR, "meeting_summary.docx"),
        "json": os.path.join(FINAL_DIR, "meeting_summary.json"),
    }
    _save_text(meeting_paths["txt"],
               "ã€æ‘˜è¦ã€‘\n" + (summary or "") +
               "\n\nã€å¤§ç¶±ã€‘\n" + "\n".join(outline or []) +
               "\n\nã€å¾…è¾¦äº‹é …ã€‘\n" + "\n".join(todos or []))
    _save_docx(meeting_paths["docx"], summary, outline, todos)
    _save_json(meeting_paths["json"], {
        "summary": summary,
        "outline": outline,
        "todos": todos
    })

    log(f"ðŸ“¦ å·²ç”¢å‡ºå ±å‘Šï¼š{meeting_paths['docx']}")
    log(f"â±ï¸ ç¸½è€—æ™‚ï¼š{time.time() - t0:.1f} ç§’")

    # âœ… ç‚º Flask app.py å›žå‚³å ±å‘Šæª”æ¡ˆè·¯å¾‘å³å¯
    return meeting_paths["docx"]
    generate_docx("output/final/meeting_summary.docx", summary, outline, todos)