# -*- coding: utf-8 -*-
"""
æ‰¹æ¬¡è™•ç†ï¼ˆé€æ®µé©—è­‰ â†’ åˆä½µ â†’ ç”¢å‡ºæœ€çµ‚æª”ï¼‰
ä¿®æ­£ç‰ˆï¼šé–å®šã€Œæœ¬æ¬¡ job çš„åˆ†æ®µè³‡æ–™å¤¾ã€ï¼Œé¿å…èª¤æŠ“æ ¹ç›®éŒ„èˆŠæª”ã€‚
"""
import os
import re
import json
import time
import hashlib
from pathlib import Path
from typing import List, Tuple, Dict, Any, Optional, Callable

# âœ… ä¿®æ­£ import ç‚º modules.xx
from modules.transcribe import transcribe
from modules.analyze import run_analysis
from modules.ai_precheck import ai_precheck  # âœ… ç¢ºä¿é€™å€‹æ¨¡çµ„æ”¾åœ¨ modules/
from docx import Document
from modules.doc_generator import generate_docx  # è‹¥ä½ æœ‰å¦ä¸€ç‰ˆå®¢è£½ DOCXï¼Œå¯é¸ç”¨

# é è¨­è³‡æ–™å¤¾ï¼ˆæ ¹ï¼‰
SEG_AUDIO_ROOT = Path("output") / "segments"
SEG_TEXT_ROOT  = Path("output") / "segments_text"
FINAL_ROOT     = Path("output") / "final"
LOGS_ROOT      = Path("output") / "logs"

for p in [SEG_AUDIO_ROOT, SEG_TEXT_ROOT, FINAL_ROOT, LOGS_ROOT]:
    p.mkdir(parents=True, exist_ok=True)


def _nkey(s: str):
    return [int(t) if t.isdigit() else t for t in re.split(r"(\d+)", s)]


def _sha256(path: Path) -> str:
    h = hashlib.sha256()
    with open(path, "rb") as f:
        for chunk in iter(lambda: f.read(1024 * 1024), b""):
            h.update(chunk)
    return h.hexdigest()


def _save_text(path: Path, text: str):
    path.parent.mkdir(parents=True, exist_ok=True)
    with open(path, "w", encoding="utf-8") as f:
        f.write(text or "")


def _save_json(path: Path, data: Dict[str, Any]):
    path.parent.mkdir(parents=True, exist_ok=True)
    with open(path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)


def _save_docx(path: Path, summary: str, outline: List[str], todos: List[str]):
    path.parent.mkdir(parents=True, exist_ok=True)
    doc = Document()
    doc.add_heading("æœƒè­°æ‘˜è¦å ±å‘Š", level=0)

    doc.add_heading("æ‘˜è¦", level=1)
    doc.add_paragraph(summary if summary else "ï¼ˆç„¡å…§å®¹ï¼‰")

    doc.add_heading("å¤§ç¶±", level=1)
    if outline:
        for item in outline:
            doc.add_paragraph(item, style="List Bullet")
    else:
        doc.add_paragraph("ï¼ˆç„¡å…§å®¹ï¼‰")

    doc.add_heading("å¾…è¾¦äº‹é …", level=1)
    if todos:
        for t in todos:
            doc.add_paragraph(t, style="List Number")
    else:
        doc.add_paragraph("ï¼ˆç„¡å…§å®¹ï¼‰")

    doc.save(str(path))


def _list_segments(seg_dir: Path) -> List[Path]:
    files = [seg_dir / f for f in os.listdir(seg_dir)
             if (seg_dir / f).is_file() and (seg_dir / f).suffix.lower() in {".wav", ".mp3", ".m4a"}]
    files.sort(key=lambda p: _nkey(p.name))
    return files


_JOB_DIR_PAT = re.compile(r"^(job_|segments_)\d{8}_\d{6}$", re.IGNORECASE)


def _find_latest_job_dir(base: Path) -> Optional[Path]:
    """
    åœ¨ base ç›®éŒ„ä¸‹å°‹æ‰¾ç¬¦åˆ job_YYYYMMDD_HHMMSS æˆ– segments_YYYYMMDD_HHMMSS çš„è³‡æ–™å¤¾ï¼Œ
    ä»¥ mtime æœ€æ–°è€…ç‚ºç›®æ¨™ã€‚
    """
    candidates = []
    for child in base.iterdir():
        if child.is_dir() and _JOB_DIR_PAT.match(child.name):
            candidates.append(child)
    if not candidates:
        return None
    candidates.sort(key=lambda p: p.stat().st_mtime, reverse=True)
    return candidates[0]


def _job_id_from_dir(path: Path) -> str:
    """
    å¾è³‡æ–™å¤¾åç¨±æ¨å° job_idï¼›è‹¥ä¸ç¬¦åˆæ…£ä¾‹å‰‡ç”¨ timestampã€‚
    """
    name = path.name
    if _JOB_DIR_PAT.match(name):
        return name
    return "job_" + time.strftime("%Y%m%d_%H%M%S")


def run_batch_process(
    segments_dir: str = str(SEG_AUDIO_ROOT),
    preview: bool = False,
    force: bool = False,
    return_progress: bool = False,
    *,
    start_index: int = 1,
    max_segments: Optional[int] = None,
    on_progress: Optional[Callable[[int, int, str], None]] = None,
) -> str:
    """
    segments_dir:
        - å»ºè­°å‚³å…¥ã€Œæœ¬æ¬¡ job çš„è³‡æ–™å¤¾ã€(ä¾‹å¦‚ output/segments/job_20251001_123009)
        - è‹¥å‚³å…¥çš„æ˜¯æ ¹ç›®éŒ„ output/segmentsï¼š
            * è‹¥åµæ¸¬åˆ°åº•ä¸‹æœ‰ job_* å­è³‡æ–™å¤¾ï¼Œæœƒè‡ªå‹•æ”¹æŠ“ã€Œæœ€æ–°çš„ jobã€(å®‰å…¨æŸµæ¬„)
            * è‹¥æ²’æœ‰ job å­è³‡æ–™å¤¾ï¼Œæ‰æœƒè™•ç†æ ¹ç›®éŒ„çš„æª”æ¡ˆï¼ˆä¸å»ºè­°ï¼‰
    """
    t0 = time.time()
    progress: List[str] = []

    def log(msg: str):
        s = time.strftime("%H:%M:%S") + " " + msg
        print(s)
        progress.append(s)

    seg_dir = Path(segments_dir).resolve()

    # âœ… å®‰å…¨æŸµæ¬„ï¼šå¦‚æœå‚³çš„æ˜¯æ ¹ç›®éŒ„ï¼Œä¸”åº•ä¸‹å­˜åœ¨ job_*ï¼Œè‡ªå‹•åˆ‡æ›åˆ°æœ€æ–° job
    if seg_dir.samefile(SEG_AUDIO_ROOT.resolve()):
        latest = _find_latest_job_dir(seg_dir)
        if latest:
            log(f"â„¹ï¸ ä½ å‚³å…¥çš„æ˜¯æ ¹ç›®éŒ„ï¼š{seg_dir}")
            log(f"â¡ï¸  è‡ªå‹•é–å®šæœ€æ–° job ç›®éŒ„ï¼š{latest.name}")
            seg_dir = latest

    if not seg_dir.exists() or not seg_dir.is_dir():
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°åˆ†æ®µç›®éŒ„ï¼š{seg_dir}")

    job_id = _job_id_from_dir(seg_dir)

    # ğŸ”’ æœ¬æ¬¡ job çš„è¼¸å‡ºè·¯å¾‘ï¼ˆé¿å…äº’ç›¸è¦†è“‹ï¼‰
    seg_text_dir = (SEG_TEXT_ROOT / job_id)
    final_dir    = (FINAL_ROOT / job_id)
    logs_dir     = (LOGS_ROOT / job_id)

    seg_text_dir.mkdir(parents=True, exist_ok=True)
    final_dir.mkdir(parents=True, exist_ok=True)
    logs_dir.mkdir(parents=True, exist_ok=True)

    # åˆ—å‡ºæœ¬æ¬¡æ®µæª”
    all_seg_files = _list_segments(seg_dir)
    if not all_seg_files:
        # å†æä¾›ä¸€æ¬¡å‹å–„æç¤ºï¼šæ˜¯å¦æŠŠæª”æ¡ˆæ”¾éŒ¯åœ°æ–¹ï¼Ÿ
        root_examples = _list_segments(SEG_AUDIO_ROOT)
        hint = ""
        if root_examples:
            hint = f"ï¼›æ³¨æ„ï¼šåœ¨æ ¹ç›®éŒ„ {SEG_AUDIO_ROOT} ç™¼ç¾ {len(root_examples)} å€‹éŸ³æª”ï¼Œè«‹ç¢ºèªæ˜¯å¦æ”¾éŒ¯ä½ç½®"
        raise FileNotFoundError(f"åœ¨ {seg_dir} æ²’æ‰¾åˆ°ä»»ä½•éŸ³æª”{hint}ã€‚")

    start = max(0, start_index - 1)
    end = None if not max_segments or max_segments <= 0 else start + max_segments
    seg_files = all_seg_files[start:end]
    total = len(seg_files)

    log(f"ğŸ”§ Job: {job_id}")
    log(f"ğŸ“‚ Segments ä¾†æºï¼š{seg_dir}")
    log(f"ğŸ“¦ æ–‡å­—è¼¸å‡ºï¼š{seg_text_dir}")
    log(f"ğŸ“¦ æœ€çµ‚è¼¸å‡ºï¼š{final_dir}")
    log(f"ğŸ”§ å°‡è™•ç†æ®µè½ï¼šå¾ç¬¬ {start_index} æ®µé–‹å§‹ï¼›æœ€å¤šè™•ç† {max_segments if max_segments else 'å…¨éƒ¨'} æ®µã€‚")
    log(f"ğŸ“Š å¯¦éš›æ®µæ•¸ï¼š{total}ï¼ˆå…¨éƒ¨å…±æœ‰ {len(all_seg_files)} æ®µï¼‰")

    manifest = {
        "job_id": job_id,
        "segments_dir": str(seg_dir),
        "created_at": time.strftime("%Y-%m-%d %H:%M:%S"),
        "preview": preview,
        "items": [],
        "start_index": start_index,
        "count": total
    }

    if on_progress:
        try:
            on_progress(0, total, "é–‹å§‹è™•ç†")
        except Exception:
            pass

    for idx, audio_path in enumerate(seg_files, 1):
        abs_index = start_index + idx - 1
        seg_id = f"{abs_index:02d}"
        base = f"seg_{seg_id}"
        paths = {
            "transcript": seg_text_dir / f"{base}_transcript.txt",
            "review":     seg_text_dir / f"{base}_review.txt",
            "revised":    seg_text_dir / f"{base}_revised.txt",
            "meta":       seg_text_dir / f"{base}_meta.json",
        }

        if (not force) and all(p.exists() for p in paths.values()):
            log(f"ğŸŸ¡ è·³é {base}ï¼ˆå·²æœ‰è¼¸å‡ºï¼‰")
            try:
                with open(paths["meta"], "r", encoding="utf-8") as f:
                    meta = json.load(f)
            except Exception:
                meta = {
                    "seg_id": seg_id,
                    "audio": Path(audio_path).name,
                    "paths": {k: str(v) for k, v in paths.items()},
                    "ok": True,
                }
            manifest["items"].append(meta)
            if on_progress:
                try:
                    on_progress(idx, total, f"è·³éç¬¬ {abs_index} æ®µ")
                except Exception:
                    pass
            continue

        log(f"ğŸ§ [{seg_id}] è™•ç†ï¼š{Path(audio_path).name}")
        try:
            transcript = transcribe(str(audio_path))
            review_text, revised_text = ai_precheck(transcript, preview=preview)

            _save_text(paths["transcript"], transcript)
            _save_text(paths["review"], review_text)
            _save_text(paths["revised"], revised_text)

            meta = {
                "seg_id": seg_id,
                "audio": Path(audio_path).name,
                "audio_sha256": _sha256(Path(audio_path)),
                "paths": {k: str(v) for k, v in paths.items()},
                "lens": {
                    "transcript": len(transcript or ""),
                    "review": len(review_text or ""),
                    "revised": len(revised_text or "")
                },
                "ok": True,
            }
            _save_json(paths["meta"], meta)
            manifest["items"].append(meta)
            log(f"âœ… [{seg_id}] å®Œæˆï¼ˆT{meta['lens']['transcript']}/R{meta['lens']['revised']}ï¼‰")
        except Exception as e:
            meta = {
                "seg_id": seg_id,
                "audio": Path(audio_path).name,
                "paths": {k: str(v) for k, v in paths.items()},
                "ok": False,
                "error": repr(e)
            }
            _save_json(paths["meta"], meta)
            manifest["items"].append(meta)
            log(f"âŒ [{seg_id}] å¤±æ•—ï¼š{e!r}")

        if on_progress:
            try:
                on_progress(idx, total, f"å®Œæˆç¬¬ {abs_index} æ®µ")
            except Exception:
                pass

    manifest_path = logs_dir / f"batch_manifest_{job_id}.json"
    _save_json(manifest_path, manifest)

    ok_items = [it for it in manifest["items"] if it.get("ok")]
    tr_paths = [Path(it["paths"]["transcript"]) for it in ok_items if it.get("paths", {}).get("transcript")]
    rvw_paths = [Path(it["paths"]["review"])     for it in ok_items if it.get("paths", {}).get("review")]
    rvd_paths = [Path(it["paths"]["revised"])    for it in ok_items if it.get("paths", {}).get("revised")]

    def _stitch(paths: List[Path], header_prefix: str) -> str:
        chunks = []
        for i, p in enumerate(paths, 1):
            name = p.name
            with open(p, "r", encoding="utf-8") as f:
                txt = f.read()
            chunks.append(f"=== {header_prefix} {i:02d} | {name} ===\n{txt}\n")
        return "\n".join(chunks)

    merged_transcript = _stitch(tr_paths, "TRANSCRIPT SEG") if tr_paths else ""
    merged_review     = _stitch(rvw_paths, "REVIEW SEG")     if rvw_paths else ""
    merged_revised    = _stitch(rvd_paths, "REVISED SEG")    if rvd_paths else ""

    final_paths = {
        "transcript_txt":         final_dir / "transcript.txt",
        "transcript_review_txt":  final_dir / "transcript_review.txt",
        "transcript_revised_txt": final_dir / "transcript_revised.txt",
    }
    _save_text(final_paths["transcript_txt"], merged_transcript)
    _save_text(final_paths["transcript_review_txt"], merged_review)
    _save_text(final_paths["transcript_revised_txt"], merged_revised)

    # åˆ†æèˆ‡è¼¸å‡ºå ±å‘Š
    summary, outline, todos = run_analysis(merged_revised, preview=False)

    meeting_paths = {
        "txt":  final_dir / "meeting_summary.txt",
        "docx": final_dir / "meeting_summary.docx",
        "json": final_dir / "meeting_summary.json",
    }
    _save_text(
        meeting_paths["txt"],
        "ã€æ‘˜è¦ã€‘\n" + (summary or "") +
        "\n\nã€å¤§ç¶±ã€‘\n" + "\n".join(outline or []) +
        "\n\nã€å¾…è¾¦äº‹é …ã€‘\n" + "\n".join(todos or [])
    )
    _save_docx(meeting_paths["docx"], summary, outline, todos)
    _save_json(meeting_paths["json"], {
        "summary": summary,
        "outline": outline,
        "todos": todos
    })

    log(f"ğŸ“¦ å·²ç”¢å‡ºå ±å‘Šï¼š{meeting_paths['docx']}")
    log(f"ğŸ§¾ Manifestï¼š{manifest_path}")
    log(f"â±ï¸ ç¸½è€—æ™‚ï¼š{time.time() - t0:.1f} ç§’")

    # âœ… ç‚º Flask app.py å›å‚³å ±å‘Šæª”æ¡ˆè·¯å¾‘å³å¯
    return str(meeting_paths["docx"])
